executor: "KubernetesExecutor"
config:
  logging:
    remote_logging: True
    logging_level: INFO
    remote_base_log_folder: s3://airflow-dags-k8s/airflow-logs # Specify the S3 bucket used for logging
    remote_log_conn_id: aws_conn # Notice that this name is used in Step3 for creating connections through Airflow UI
    delete_worker_pods: False
    encrypt_s3_logs: True
airflow:
  config:
    AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "aws_default"
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://airflow-dags-k8s/airflow-logs"
    AIRFLOW__LOGGING__LOG_LEVEL: "INFO"
    AIRFLOW__LOGGING__S3_LOG_FOLDER: "s3://airflow-dags-k8s/airflow-logs"
    AIRFLOW__LOGGING__DELETE_LOCAL_LOGS: "False"
  logs:
    remote_logging: True
    remote_log_conn_id: aws_default  # The connection ID from Step 1
    remote_base_log_folder: s3://airflow-dags-k8s/airflow-logs
    logging_level: INFO  # Set logging level to INFO, but can also set to "ERROR"
    delete_local_logs: false
dags:  
  gitSync:
    enabled: true
    # Git repository URL
    repo: https://github.com/shadab-bigdel/airflow.git
    branch: main
    # The Git revision to check out (branch, tag, or hash)
    ref: main
    depth: 1
    # Maximum number of consecutive failures allowed before aborting
    maxFailures: 0
    # Subpath within the repo where DAGs are located
    # Should be "" if DAGs are at the repo root
    subPath: "dags"
